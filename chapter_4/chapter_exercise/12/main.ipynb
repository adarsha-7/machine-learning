{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "818dc90f",
   "metadata": {},
   "source": [
    "Implement batch gradient descent with early stopping for softmax regression without using Scikit-Learn, only NumPy. Use it on a classification task such as the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe8289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4410a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_bias = np.c_[np.ones(len(X)), X]\n",
    "\n",
    "y_one_hot = np.zeros((len(y), 3))\n",
    "y_one_hot[np.arange(len(y)), y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0ed348c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "total_size = len(X)\n",
    "test_size = int(test_ratio * total_size)\n",
    "validation_size = int(validation_ratio * total_size)\n",
    "train_size = total_size - test_size - validation_size\n",
    "\n",
    "random_indices = np.random.permutation(total_size)\n",
    "\n",
    "X_train = X_with_bias[random_indices[:train_size]]\n",
    "y_train = y_one_hot[random_indices[:train_size]]\n",
    "\n",
    "X_valid = X_with_bias[random_indices[train_size:train_size+validation_size]]\n",
    "y_valid = y_one_hot[random_indices[train_size:train_size+validation_size]]\n",
    "\n",
    "X_test = X_with_bias[random_indices[train_size+validation_size:]]\n",
    "y_test = y_one_hot[random_indices[train_size+validation_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "9a8031b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X_train[:, 1:].mean(axis=0)\n",
    "std = X_train[:, 1:].std(axis=0)\n",
    "X_train[:, 1:] = (X_train[:, 1:] - mean) / std\n",
    "X_valid[:, 1:] = (X_valid[:, 1:] - mean) / std\n",
    "X_test[:, 1:] = (X_test[:, 1:] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c4be5d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 0% Accuracy: 0.000%\n",
      "Training data: 10% Accuracy: 86.667%\n",
      "Training data: 20% Accuracy: 86.667%\n",
      "Training data: 30% Accuracy: 90.000%\n",
      "Training data: 40% Accuracy: 93.333%\n",
      "Training data: 50% Accuracy: 93.333%\n",
      "Training data: 60% Accuracy: 93.333%\n",
      "Training data: 70% Accuracy: 93.333%\n",
      "Training data: 80% Accuracy: 93.333%\n",
      "Training data: 90% Accuracy: 93.333%\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1 \n",
    "n_epochs = 1000\n",
    "alpha = 0.01\n",
    "\n",
    "m = len(X_train)\n",
    "inputs = len(X_train[0])\n",
    "outputs = len(np.unique(y))\n",
    " \n",
    "theta = np.random.randn(inputs, outputs)\n",
    "\n",
    "def probability(softmax_scores):\n",
    "    exps = np.exp(softmax_scores)\n",
    "    exp_sums = exps.sum(axis=1, keepdims=True)\n",
    "    return exps / exp_sums\n",
    "\n",
    "def probability_k(softmax_scores, k):\n",
    "    return (np.exp(softmax_scores[k])) / sum([np.exp(s) for s in softmax_scores])\n",
    "\n",
    "def accuracy(theta):\n",
    "    sm_scores = X_valid @ theta\n",
    "    y_proba = probability(sm_scores)\n",
    "    y_predict = y_proba.argmax(axis=1)\n",
    "    y_valid_1d = y_valid.argmax(axis=1)\n",
    "\n",
    "    return (y_predict == y_valid_1d).mean()\n",
    "\n",
    "checkpoint = int(n_epochs / 10)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    if epoch % checkpoint == 0:\n",
    "        print(f\"Training data: {epoch / n_epochs * 100 :.0f}%\",\n",
    "              f\"Accuracy: {accuracy(theta) * 100 :.3f}%\")\n",
    "        \n",
    "    softmax_scores = X_train @ theta\n",
    "    y_proba = probability(softmax_scores)\n",
    "    reg_gradient = alpha * theta\n",
    "    gradients = (1 / m) * X_train.T @ (y_proba - y_train) + reg_gradient\n",
    "    theta = theta - (eta * gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ca0529bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.39072784,  0.96005921, -1.605765  ],\n",
       "       [-0.7154986 ,  0.26480741,  0.48918194],\n",
       "       [ 1.20162228, -0.25405516, -0.24278623],\n",
       "       [-1.30635205,  0.56604163,  1.42493294],\n",
       "       [-1.41502116, -0.57502337,  2.02946397]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8e48a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_scores = X_valid @ theta\n",
    "y_proba = probability(sm_scores)\n",
    "y_predict = y_proba.argmax(axis=1)\n",
    "y_valid_2 = y_valid.argmax(axis=1)\n",
    "\n",
    "acc = (y_predict == y_valid_2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ce994a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9333333333333333)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
